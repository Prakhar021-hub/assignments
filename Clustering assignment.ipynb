{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7ed422-bbae-44ef-aba6-8499bb84b7d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Unsupervised Learning: Clustering Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What is unsupervised learning in the context of machine learning?\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the model learns from **unlabeled data**. It aims to find hidden patterns or intrinsic structures in the input data.\n",
    "\n",
    "**Example:** Clustering customers based on purchasing behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How does K-Means clustering algorithm work?\n",
    "\n",
    "K-Means works by:\n",
    "\n",
    "1. Selecting `k` initial centroids.\n",
    "2. Assigning each data point to the nearest centroid.\n",
    "3. Recomputing centroids based on assigned points.\n",
    "4. Repeating steps 2–3 until convergence.\n",
    "\n",
    "**Example:**\n",
    "Clustering animals based on weight and height.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Explain the concept of a dendrogram in hierarchical clustering.\n",
    "\n",
    "A **dendrogram** is a tree-like diagram that shows the merging process in hierarchical clustering. It visualizes how clusters are formed at each step and helps choose the optimal number of clusters by cutting the tree.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What is the main difference between K-Means and Hierarchical Clustering?\n",
    "\n",
    "* **K-Means** requires the number of clusters beforehand and is efficient for large datasets.\n",
    "* **Hierarchical clustering** builds a hierarchy of clusters and doesn't need the number of clusters upfront.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What are the advantages of DBSCAN over K-Means?\n",
    "\n",
    "* Does **not require** the number of clusters beforehand.\n",
    "* Can detect **arbitrarily shaped** clusters.\n",
    "* Can **identify noise/outliers** naturally.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. When would you use Silhouette Score in clustering?\n",
    "\n",
    "Silhouette Score is used to **evaluate clustering quality**. It measures how similar an object is to its own cluster vs. other clusters (range: -1 to 1).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What are the limitations of Hierarchical Clustering?\n",
    "\n",
    "* **Scalability:** Computationally expensive for large datasets.\n",
    "* **Irreversible:** Once merged or split, operations cannot be undone.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Why is feature scaling important in clustering algorithms like K-Means?\n",
    "\n",
    "K-Means uses **Euclidean distance**, so features with larger scales dominate. **Standardization or normalization** ensures fair contribution from all features.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. How does DBSCAN identify noise points?\n",
    "\n",
    "DBSCAN labels points as **noise** if they do not belong to any cluster, i.e., they are not within `eps` distance of a core point and not reachable.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Define inertia in the context of K-Means.\n",
    "\n",
    "Inertia is the **sum of squared distances** between each point and its cluster centroid. Lower inertia means tighter clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. What is the elbow method in K-Means clustering?\n",
    "\n",
    "The **elbow method** plots the number of clusters (k) vs. inertia. The \"elbow\" point where the inertia stops decreasing significantly suggests the optimal number of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Describe the concept of \"density\" in DBSCAN.\n",
    "\n",
    "In DBSCAN, density refers to the number of points within a radius (`eps`). A region with **enough points (minPts)** is considered a **dense region**, forming a cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Can hierarchical clustering be used on categorical data?\n",
    "\n",
    "Yes, but it requires a suitable **distance metric for categorical variables**, such as Hamming or Jaccard distance, or encoding before using Euclidean distance.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. What does a negative Silhouette Score indicate?\n",
    "\n",
    "A negative Silhouette Score indicates that a point is **closer to a different cluster** than to the one it was assigned — implying **poor clustering**.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
    "\n",
    "Linkage criteria determine how the **distance between clusters** is calculated:\n",
    "\n",
    "* **Single:** minimum distance\n",
    "* **Complete:** maximum distance\n",
    "* **Average:** average distance\n",
    "* **Ward's:** minimizes variance\n",
    "\n",
    "---\n",
    "\n",
    "### 16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
    "\n",
    "K-Means assumes clusters are **spherical and of similar size**. It struggles with:\n",
    "\n",
    "* Varying densities\n",
    "* Non-convex shapes\n",
    "* Unequal cluster sizes\n",
    "\n",
    "---\n",
    "\n",
    "### 17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
    "\n",
    "* **eps:** maximum radius of neighborhood\n",
    "* **minPts:** minimum number of points to form a cluster\n",
    "\n",
    "They affect how clusters and noise points are defined.\n",
    "\n",
    "---\n",
    "\n",
    "### 18. How does K-Means++ improve upon standard K-Means initialization?\n",
    "\n",
    "K-Means++ selects initial centroids in a **smart way** to spread them out, improving:\n",
    "\n",
    "* Convergence speed\n",
    "* Final cluster quality\n",
    "\n",
    "---\n",
    "\n",
    "### 19. What is agglomerative clustering?\n",
    "\n",
    "Agglomerative clustering is a **bottom-up hierarchical method**:\n",
    "\n",
    "* Start with each point as its own cluster\n",
    "* Merge closest clusters iteratively until all points are in one cluster or a stopping criterion is met\n",
    "\n",
    "---\n",
    "\n",
    "### 20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
    "\n",
    "* **Inertia** only measures compactness.\n",
    "* **Silhouette Score** considers both **cohesion and separation**, giving a more holistic view of clustering quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb1dad-4edb-40ba-a5e9-40ba9a2ffe52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
